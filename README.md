College-Prediction-Model <br>

Folder Structure <br>
College_Admission_Prediction/ <br>
â”‚ <br>
â”œâ”€â”€ data/ <br>
â”‚ â””â”€â”€ admission_predict.csv <br>
â”‚ <br>
â”œâ”€â”€ notebooks/ <br>
â”‚ â””â”€â”€ College_Admission_Prediction.ipynb <br>
â”‚ <br>
â”œâ”€â”€ src/ <br>
â”‚ â”œâ”€â”€ train_model.py <br>
â”‚ â”œâ”€â”€ visualize_data.py <br>
â”‚ â””â”€â”€ init.py <br>
â”‚<br>
â”œâ”€â”€ results/ <br>
â”‚ â”œâ”€â”€ regression_actual_vs_predicted.png <br>
â”‚ â”œâ”€â”€ calibration_curve.png <br>
â”‚ â”œâ”€â”€ pdp_GRE.png <br>
â”‚ â”œâ”€â”€ pdp_CGPA.png <br>
â”‚ â”œâ”€â”€ linear_coefficients.csv <br>
â”‚ â””â”€â”€ model_comparison.csv <br>
â”‚ <br>
â”œâ”€â”€ requirements.txt <br>
â”œâ”€â”€ README.md <br>
â””â”€â”€ .gitignore <br>

** Dataset Information ** <br>

File: admission_predict.csv <br>
<br>
Columns: <br>
<br>
Column Description: <br>
GRE Score â€” Graduate Record Examination (0â€“340) <br>
TOEFL Score â€” English proficiency score (0â€“120) <br>
University Rating â€” Reputation of the university (1â€“5) <br>
SOP â€” Statement of Purpose strength (1â€“5) <br>
LOR â€” Letter of Recommendation strength (1â€“5) <br>
CGPA â€” Cumulative GPA (0â€“10) <br>
Research â€” 1 = Yes, 0 = No <br>
Chance of Admit â€” Target variable (0â€“1) <br>
<br>

** How to Run on Google Colab ** <br>

Open the notebook link above or upload College_Admission_Prediction.ipynb to Colab. <br>
<br>
Mount your Google Drive to load the dataset: <br>
<br>
from google.colab import drive <br>
drive.mount('/content/drive') <br>
df = pd.read_csv('/content/drive/MyDrive/admission_predict.csv') <br>
<br>
Run all cells sequentially: <br>
Data Loading & Cleaning <br>
Visualization <br>
Model Training & Evaluation <br>
Prediction & Fairness Check <br>

** Project Workflow ** <br>

Exploratory Data Analysis â€” View shape, info, statistics, and nulls <br>

Visualization â€” Histograms for GRE, TOEFL, CGPA, etc. <br>

Data Cleaning â€” Drop unused columns, handle missing values <br>

Regression Modeling â€” Train models (Linear, Lasso, Random Forest) using GridSearchCV <br>

Calibration â€” Apply CalibratedClassifierCV to improve probability estimates <br>

Threshold Policy â€” Define cost-sensitive thresholds for admission decisions <br>

Fairness Metrics â€” Evaluate subgroups (Research/University Rating) for bias <br>

Interpretability â€” Coefficients, Partial Dependence Plots (PDPs) <br>

Evaluation â€” Report RMSE, MAE, RÂ², ROC-AUC, Brier Score <br>

Prediction â€” Compute real-world admission probability <br>

** ðŸ“ˆ Results Summary ** <br>

| Model | Best Parameters | Cross-Val RÂ² | <br>
|--------|-----------------|--------------| <br>
| Linear Regression | â€” | 0.805 | <br>
| Lasso Regression | Î± = 0.1 | 0.79 | <br>
| Random Forest | n_estimators = 200, max_depth = 10 | 0.78 | <br>
<br>

Regression Metrics (on test set): <br>
RMSE â‰ˆ 0.0562 <br>
MAE â‰ˆ 0.0399 <br>
RÂ² â‰ˆ 0.8520 <br>
<br>

Calibration & Classification Metrics: <br>
ROC-AUC â‰ˆ 0.9654 <br>
Brier Score â‰ˆ 0.0806 <br>

<br>

Subgroup Fairness Metrics (Accuracy): <br>
Research=0 â†’ 0.886 <br>
Research=1 â†’ 0.857 <br>
UnivRating=5 â†’ 0.957 <br>
UnivRating=3 â†’ 0.839 <br>
<br>

** Final Model: Linear Regression + Calibrated Random Forest (best generalization & probability calibration)** <br>
** Example Predictions ** <br>

Input format: GRE, TOEFL, University Rating, SOP, LOR, CGPA, Research <br>
model.predict([[337, 118, 4, 4.5, 4.5, 9.65, 0]]) <br>
â†’ Predicted Admission Chance â‰ˆ 93.2 % <br>
<br>
model.predict([[320, 113, 2, 2.0, 2.5, 8.64, 1]]) <br>
â†’ Predicted Admission Chance â‰ˆ 72.4 % <br>

** Visualizations Included ** <br>

GRE Distribution <br>
TOEFL Distribution <br>
University Rating, SOP, LOR, CGPA Histograms <br>
Research Count Distribution <br>
Regression Actual vs Predicted Scatter <br>
Calibration Curve (Isotonic Regression) <br>
Partial Dependence Plots for GRE & CGPA <br>
Linear Regression Coefficients (Feature Importance) <br>

Each visualization provides insight into data patterns, prediction quality, and fairness across subgroups. <br>

** Future Enhancements ** <br>

Add SHAP-based feature importance for deeper interpretability <br>
Implement advanced boosting (XGBoost, LightGBM) models <br>
Deploy via Streamlit / Flask for live prediction <br>
Add Optuna or Bayesian Optimization for hyperparameter tuning <br>
Automate fairness dashboards using Plotly / Dash <br>
Integrate continuous retraining pipeline with version tracking <br>

** Author ** <br>

Monarch Baweja <br>
Goa Institute of Management <br>
GitHub: monarchbaweja1
 <br>

** License ** <br>

MIT License â€” Open for educational and research use. <br>
