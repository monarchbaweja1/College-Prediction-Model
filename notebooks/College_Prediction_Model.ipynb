{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP5EeFe2Lq3Y"
      },
      "outputs": [],
      "source": [
        "# Importing essential libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxvkLGAtLq3c"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "df = pd.read_csv('/content/admission_predict.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGuDGsgCLq3e"
      },
      "source": [
        "## Exploring the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzrGn_loLq3g"
      },
      "outputs": [],
      "source": [
        "# Returns number of rows and columns of the dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3YYezJuLq3k"
      },
      "outputs": [],
      "source": [
        "# Returns the first x number of rows when head(num). Without a number it returns 5\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzQYUHbxLq3l"
      },
      "outputs": [],
      "source": [
        "# Returns the first x number of rows when tail(num). Without a number it returns 5\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK2H5BKqLq3l"
      },
      "outputs": [],
      "source": [
        "# Returns an object with all of the column headers\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjLwwdGXLq3l"
      },
      "outputs": [],
      "source": [
        "# Returns basic information on all columns\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0ZqGOtGLq3m"
      },
      "outputs": [],
      "source": [
        "# Returns basic statistics on numeric columns\n",
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVidW78_Lq3n"
      },
      "outputs": [],
      "source": [
        "# Returns different datatypes for each columns (float, int, string, bool, etc.)\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgs4gMYeLq3n"
      },
      "outputs": [],
      "source": [
        "# Returns true for a column having null values, else false\n",
        "df.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "S_12dKdhLq3n"
      },
      "outputs": [],
      "source": [
        "# Renaming the columns with appropriate names\n",
        "df = df.rename(columns={'GRE Score': 'GRE', 'TOEFL Score': 'TOEFL', 'LOR ': 'LOR', 'Chance of Admit ': 'Probability'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc4IK5LjLq3o"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_ZXyKHLq3o"
      },
      "outputs": [],
      "source": [
        "# Visualizing the feature GRE\n",
        "fig = plt.hist(df['GRE'], rwidth=0.7)\n",
        "plt.title(\"Distribution of GRE Scores\")\n",
        "plt.xlabel('GRE Scores')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Xcv0OBixLq3o"
      },
      "outputs": [],
      "source": [
        "# Visualizing the feature TOEFL\n",
        "fig = plt.hist(df['TOEFL'], rwidth=0.7)\n",
        "plt.title('Distribution of TOEFL Scores')\n",
        "plt.xlabel('TOEFL Scores')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaEokjS5Lq3p"
      },
      "outputs": [],
      "source": [
        "# Visualizing the feature TOEFL\n",
        "fig = plt.hist(df['University Rating'], rwidth=0.7)\n",
        "plt.title('Distribution of University Rating')\n",
        "plt.xlabel('University Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kmq5wb3yLq3p"
      },
      "outputs": [],
      "source": [
        "# Visualizing the feature TOEFL\n",
        "fig = plt.hist(df['SOP'], rwidth=0.7)\n",
        "plt.title('Distribution of SOP')\n",
        "plt.xlabel('SOP Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2P51eodLq3p"
      },
      "outputs": [],
      "source": [
        "# Visualizing the feature TOEFL\n",
        "fig = plt.hist(df['LOR'], rwidth=0.7)\n",
        "plt.title('Distribution of LOR Rating')\n",
        "plt.xlabel('LOR Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5o1JeZULq3p"
      },
      "outputs": [],
      "source": [
        "# Visualizing the feature TOEFL\n",
        "fig = plt.hist(df['CGPA'], rwidth=0.7)\n",
        "plt.title('Distribution of CGPA')\n",
        "plt.xlabel('CGPA')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zeh8Uyp6Lq3p"
      },
      "outputs": [],
      "source": [
        "# Visualizing the feature TOEFL\n",
        "fig = plt.hist(df['Research'], rwidth=0.7)\n",
        "plt.title('Distribution of Research Papers')\n",
        "plt.xlabel('Research')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A838n0A7Lq3p"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY9RaFg1Lq3p"
      },
      "outputs": [],
      "source": [
        "# Removing the serial no, column\n",
        "df.drop('Serial No.', axis='columns', inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SelS4zLALq3q"
      },
      "outputs": [],
      "source": [
        "# Replacing the 0 values from ['GRE','TOEFL','University Rating','SOP','LOR','CGPA'] by NaN\n",
        "df_copy = df.copy(deep=True)\n",
        "df_copy[['GRE','TOEFL','University Rating','SOP','LOR','CGPA']] = df_copy[['GRE','TOEFL','University Rating','SOP','LOR','CGPA']].replace(0, np.nan)\n",
        "df_copy.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREVIOUSLY APPLIED WITH DECISION FOREST ONLY\n"
      ],
      "metadata": {
        "id": "fiCloZT5Z_Pa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ut3V800QbCCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "MBxkQiIJaFI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove extra spaces in all column names\n",
        "df.columns = df.columns.str.strip()\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "6Ils0lSqam6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.copy() # Create a copy to avoid modifying the original df again\n",
        "\n",
        "X = data.drop(['Probability'], axis=1)\n",
        "y = data['Probability']"
      ],
      "metadata": {
        "id": "GHN5kXrYbNKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a9b4528"
      },
      "source": [
        "The next cell splits the data into training and testing sets, which is necessary before training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vj5yTCG_bMxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "oMljzG4eaErl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Accuracy (RÂ² Score):\", round(r2*100, 2), \"%\")\n",
        "\n",
        "# Example Prediction\n",
        "print('Chance of getting into UCLA is {}%'.format(\n",
        "    round(model.predict([[337, 118, 4, 4.5, 4.5, 9.65, 0]])[0]*100, 2)\n",
        "))\n"
      ],
      "metadata": {
        "id": "cqAx_pafbnPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75f251bc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting actual vs predicted values for Random Forest Regression\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Random Forest Regression: Actual vs Predicted\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L8-iskoLq3q"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca4ifddYLq3q"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset in features and label\n",
        "X = df_copy.drop('Probability', axis='columns')\n",
        "y = df_copy['Probability']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeO30_h3Lq3q"
      },
      "outputs": [],
      "source": [
        "# Using GridSearchCV to find the best algorithm for this problem\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEW__z8HLq3q"
      },
      "outputs": [],
      "source": [
        "# Creating a function to calculate best model for this problem\n",
        "def find_best_model(X, y):\n",
        "    models = {\n",
        "        'linear_regression': {\n",
        "            'model': LinearRegression(),\n",
        "            'parameters': {\n",
        "                # 'normalize': [True,False] # Removed the normalize parameter\n",
        "            }\n",
        "        },\n",
        "\n",
        "        'lasso': {\n",
        "            'model': Lasso(),\n",
        "            'parameters': {\n",
        "                'alpha': [1,2],\n",
        "                'selection': ['random', 'cyclic']\n",
        "            }\n",
        "        },\n",
        "\n",
        "        'svr': {\n",
        "            'model': SVR(),\n",
        "            'parameters': {\n",
        "                'gamma': ['auto','scale']\n",
        "            }\n",
        "        },\n",
        "\n",
        "        'decision_tree': {\n",
        "            'model': DecisionTreeRegressor(),\n",
        "            'parameters': {\n",
        "                'criterion': ['squared_error', 'friedman_mse'],\n",
        "                'splitter': ['best', 'random']\n",
        "            }\n",
        "        },\n",
        "\n",
        "        'random_forest': {\n",
        "            'model': RandomForestRegressor(criterion='squared_error'),\n",
        "            'parameters': {\n",
        "                'n_estimators': [5,10,15,20]\n",
        "            }\n",
        "        },\n",
        "\n",
        "        'knn': {\n",
        "            'model': KNeighborsRegressor(algorithm='auto'),\n",
        "            'parameters': {\n",
        "                'n_neighbors': [2,5,10,20]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    scores = []\n",
        "    for model_name, model_params in models.items():\n",
        "        gs = GridSearchCV(model_params['model'], model_params['parameters'], cv=5, return_train_score=False)\n",
        "        gs.fit(X, y)\n",
        "        scores.append({\n",
        "            'model': model_name,\n",
        "            'best_parameters': gs.best_params_,\n",
        "            'score': gs.best_score_\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(scores, columns=['model','best_parameters','score'])\n",
        "\n",
        "find_best_model(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3307cf79"
      },
      "source": [
        "The next cell splits the data into training and testing sets, which is necessary before training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ecdedf2"
      },
      "source": [
        "# Splitting the dataset into train and test samples\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb1dec14"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting actual vs predicted values for Linear Regression\n",
        "y_pred_lr = model.predict(X_test)\n",
        "plt.scatter(y_test, y_pred_lr, alpha=0.6)\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Linear Regression: Actual vs Predicted\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWu6kDdjLq3q"
      },
      "source": [
        "#### Since the Linear Regression algorithm has the highest accuracy, the model selected for this problem is Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DY_ULFRLq3r"
      },
      "outputs": [],
      "source": [
        "# Using cross_val_score for gaining highest accuracy\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(LinearRegression(), X, y, cv=5) # Removed normalize=True\n",
        "print('Highest Accuracy : {}%'.format(round(sum(scores)*100/len(scores)), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pXlpsypLq3r"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into train and test samples\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
        "print(len(X_train), len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6WrYKLrLq3r"
      },
      "outputs": [],
      "source": [
        "# Creating Linear Regression Model\n",
        "model = LinearRegression() # Removed normalize=True\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7ZuWKt-Lq3r"
      },
      "source": [
        "#### Predicting the values using our trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RPKeHC0XLq3r"
      },
      "outputs": [],
      "source": [
        "# Prediction 1\n",
        "# Input in the form : GRE, TOEFL, University Rating, SOP, LOR, CGPA, Research\n",
        "print('Chance of getting into UCLA is {}%'.format(round(model.predict([[337, 118, 4, 4.5, 4.5, 9.65, 0]])[0]*100, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fliCLT7-Lq3s"
      },
      "outputs": [],
      "source": [
        "# Prediction 2\n",
        "# Input in the form : GRE, TOEFL, University Rating, SOP, LOR, CGPA, Research\n",
        "print('Chance of getting into UCLA is {}%'.format(round(model.predict([[320, 113, 2, 2.0, 2.5, 8.64, 1]])[0]*100, 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOW WE TRIED VARIOUS ALGORITHM AND NOW ACCURACY HAS IMPROVED"
      ],
      "metadata": {
        "id": "uRNUrXemb9IA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}